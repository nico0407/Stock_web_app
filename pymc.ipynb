{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72783909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CCC-GARCH in PyMC (joint estimation) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import aesara\n",
    "import aesara.tensor as at\n",
    "import arviz as az\n",
    "\n",
    "# Inputs expected:\n",
    "# df:            (T x n) DataFrame of observed series (rows=dates, cols=series ids)\n",
    "# df_cond_vol:   (T x n) optional conditional std devs to seed sigma^2 at the first step\n",
    "\n",
    "# ------------- Prepare data -------------\n",
    "df = df.sort_index()\n",
    "Y = df.values.astype(np.float64)  # shape (T, n)\n",
    "dates = df.index\n",
    "series_names = df.columns\n",
    "T, n = Y.shape\n",
    "if T < 3:\n",
    "    raise ValueError(\"Need at least 3 observations because σ_t uses X_{t-2}.\")\n",
    "\n",
    "# Optional seed from prefit vols\n",
    "use_seed_sigma = False\n",
    "sigma2_seed = None\n",
    "try:\n",
    "    if isinstance(df_cond_vol, pd.DataFrame):\n",
    "        df_cond_vol = df_cond_vol.loc[dates, series_names]\n",
    "        sigma2_seed = (df_cond_vol.iloc[0].values.astype(np.float64)) ** 2  # (n,)\n",
    "        use_seed_sigma = True\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "Y_shared = aesara.shared(Y)\n",
    "\n",
    "with pm.Model() as ccc_garch:\n",
    "    # ---------- Priors per series ----------\n",
    "    c     = pm.Normal(\"c\",   mu=100.0, sigma=20.0, shape=n)    # c_i\n",
    "    phi   = pm.Normal(\"phi\", mu=0.0,   sigma=0.5, shape=n)     # φ_i\n",
    "    omega = pm.TruncatedNormal(\"omega\", mu=100.0, sigma=20.0, lower=1e-8, shape=n)  # ω_i > 0\n",
    "    alpha = pm.Beta(\"alpha\", alpha=5.0, beta=2.0, shape=n)     # α_i ∈ (0,1)\n",
    "    beta  = pm.Beta(\"beta\",  alpha=2.0, beta=5.0, shape=n)     # β_i ∈ (0,1)\n",
    "\n",
    "    # Soft stationarity: α + β < 1\n",
    "    pm.Potential(\"stationarity_soft\",\n",
    "        -100.0 * at.sum(at.nnet.relu(alpha + beta - 0.999))\n",
    "    )\n",
    "\n",
    "    # ---------- Constant correlation for z_t ----------\n",
    "    # Cholesky factor of a correlation matrix via LKJ prior (more efficient than dense cov)\n",
    "    lkj = pm.LKJCholeskyCov.dist(\n",
    "        n=n,\n",
    "        eta=2.0,              # mild shrinkage to identity\n",
    "        sd_dist=pm.HalfNormal.dist(1.0)  # standard deviations; here they’ll be 1.0 in expectation\n",
    "    )\n",
    "    L = pm.RandomVariable(\"L\", lkj)      # lower-triangular Cholesky of covariance\n",
    "    # Because z are standardized, covariance = correlation; using L as 'chol' is appropriate.\n",
    "    # If you prefer, you can also get the full correlation matrix via:\n",
    "    # R = pm.Deterministic(\"R\", L @ L.T)\n",
    "\n",
    "    # ---------- AR(1)-GARCH(1,1) recursion ----------\n",
    "    # μ_{i,t} = c_i + φ_i X_{i,t-1}\n",
    "    # σ^2_{i,t} = ω_i + α_i (X_{i,t-1} - c_i - X_{i,t-2})^2 + β_i σ^2_{i,t-1}\n",
    "\n",
    "    # Initial variance at t=1\n",
    "    if use_seed_sigma:\n",
    "        sigma2_init = at.as_tensor_variable(sigma2_seed)  # (n,)\n",
    "    else:\n",
    "        denom = at.clip(1.0 - alpha - beta, 1e-6, 1e6)    # avoid division by tiny numbers\n",
    "        sigma2_init = omega / denom                       # (n,)\n",
    "\n",
    "    def step(y_tm1, y_tm2, sigma2_tm1, c, phi, omega, alpha, beta):\n",
    "        mu_t = c + phi * y_tm1\n",
    "        arch_term = (y_tm1 - c - y_tm2) ** 2\n",
    "        sigma2_t = omega + alpha * arch_term + beta * sigma2_tm1\n",
    "        sigma2_t = at.clip(sigma2_t, 1e-12, 1e12)         # numeric safety\n",
    "        return mu_t, sigma2_t\n",
    "\n",
    "    y_tm1_seq = Y_shared[1:]   # (T-1, n)\n",
    "    y_tm2_seq = Y_shared[:-1]  # (T-1, n)\n",
    "\n",
    "    (mu_seq, sigma2_seq), _ = aesara.scan(\n",
    "        fn=step,\n",
    "        sequences=[y_tm1_seq, y_tm2_seq],\n",
    "        outputs_info=[None, sigma2_init],\n",
    "        non_sequences=[c, phi, omega, alpha, beta]\n",
    "    )\n",
    "    sigma_seq = at.sqrt(sigma2_seq)                       # (T-1, n)\n",
    "    z_seq = (Y_shared[1:] - mu_seq) / sigma_seq           # (T-1, n)\n",
    "\n",
    "    # ---------- Likelihood (CCC part) ----------\n",
    "    # Each row z_t ~ MVN(0, R) with Cholesky L; i.i.d. over time given params.\n",
    "    pm.MvNormal(\"z\",\n",
    "                mu=at.zeros(n),\n",
    "                chol=L,\n",
    "                observed=z_seq)\n",
    "\n",
    "    # ---------- Inference ----------\n",
    "    trace = pm.sample(draws=1500, tune=1500, chains=4, target_accept=0.9, random_seed=123)\n",
    "\n",
    "    # Optionally save μ_t and σ_t sequences\n",
    "    pm.Deterministic(\"mu_seq\", mu_seq)\n",
    "    pm.Deterministic(\"sigma_seq\", sigma_seq)\n",
    "\n",
    "    ppc = pm.sample_posterior_predictive(trace, var_names=[\"z\"])\n",
    "\n",
    "# Quick summaries\n",
    "print(az.summary(trace, var_names=[\"c\",\"phi\",\"omega\",\"alpha\",\"beta\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2739863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Two-step CCC-GARCH in PyMC: estimate only the constant correlation R ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import aesara.tensor as at\n",
    "import arviz as az\n",
    "\n",
    "# Inputs you already have:\n",
    "# df            : (T x n) observed series, rows=dates, cols=series ids\n",
    "# df_cond_mean  : (T x n) conditional means from your univariate AR(1)\n",
    "# df_cond_vol   : (T x n) conditional std devs from your univariate GARCH(1,1)\n",
    "\n",
    "# 1) Align and standardize\n",
    "df      = df.sort_index()\n",
    "df_cond_mean = df_cond_mean.loc[df.index, df.columns]\n",
    "df_cond_vol  = df_cond_vol.loc[df.index, df.columns]\n",
    "\n",
    "# guard against zero/neg vols (shouldn't happen, but be safe numerically)\n",
    "eps = 1e-12\n",
    "S   = np.maximum(df_cond_vol.values.astype(float), eps)          # (T, n)\n",
    "M   = df_cond_mean.values.astype(float)                          # (T, n)\n",
    "Y   = df.values.astype(float)                                    # (T, n)\n",
    "\n",
    "Z   = (Y - M) / S                                                # standardized residuals (T, n)\n",
    "\n",
    "# If your univariate filters use lags, the first row(s) may be unreliable.\n",
    "# Common to drop the first 1–2 rows; adjust k as you see fit.\n",
    "k = 1\n",
    "Z_use = Z[k:, :]                                                 # (T-k, n)\n",
    "T_eff, n = Z_use.shape\n",
    "\n",
    "# 2) PyMC model: z_t ~ MVN(0, R) i.i.d. over t, with R constant (CCC)\n",
    "with pm.Model() as ccc_step2:\n",
    "    # Easiest: sample correlation matrix directly (valid & PD)\n",
    "    R = pm.LKJCorr(\"R\", n=n, eta=2.0)                            # mild shrinkage to I\n",
    "\n",
    "    # Likelihood on standardized residuals\n",
    "    pm.MvNormal(\"z\",\n",
    "                mu=at.zeros(n),\n",
    "                cov=R,                                           # correlation works as covariance here\n",
    "                observed=Z_use)\n",
    "\n",
    "    trace_R = pm.sample(draws=2000, tune=2000, chains=4,\n",
    "                        target_accept=0.9, random_seed=123)\n",
    "\n",
    "# 3) Quick summaries\n",
    "print(az.summary(trace_R, var_names=[\"R\"]))\n",
    "\n",
    "# Example: posterior mean correlation matrix\n",
    "R_mean = trace_R.posterior[\"R\"].mean(dim=(\"chain\",\"draw\")).values\n",
    "R_df = pd.DataFrame(R_mean, index=df.columns, columns=df.columns)\n",
    "print(\"\\nPosterior mean R:\\n\", R_df.round(3))\n",
    "\n",
    "# Optional: compare to the empirical correlation of Z_use\n",
    "emp_corr = pd.DataFrame(Z_use).corr().values\n",
    "emp_corr_df = pd.DataFrame(emp_corr, index=df.columns, columns=df.columns)\n",
    "print(\"\\nEmpirical corr of standardized residuals:\\n\", emp_corr_df.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fffb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6027f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint CCC-GARCH with time-updating mu_t, sigma_t and constant correlation R\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import aesara\n",
    "import aesara.tensor as at\n",
    "import arviz as az\n",
    "\n",
    "# --- Inputs ---\n",
    "# df: (T x n) DataFrame of the observed series (rows=dates, cols=series ids)\n",
    "# OPTIONAL: df_cond_vol to seed the initial variance (first row used)\n",
    "\n",
    "# Prepare data\n",
    "df = df.sort_index()\n",
    "Y = df.values.astype(np.float64)         # (T, n)\n",
    "dates = df.index\n",
    "series = df.columns\n",
    "T, n = Y.shape\n",
    "if T < 3:\n",
    "    raise ValueError(\"Need at least 3 observations (variance uses X_{t-2}).\")\n",
    "\n",
    "# Optional seeding of initial variance from univariate fits\n",
    "use_seed_sigma = False\n",
    "sigma2_seed = None\n",
    "try:\n",
    "    if isinstance(df_cond_vol, pd.DataFrame):\n",
    "        df_cond_vol = df_cond_vol.loc[dates, series]\n",
    "        sigma2_seed = (df_cond_vol.iloc[0].values.astype(np.float64)) ** 2  # (n,)\n",
    "        use_seed_sigma = True\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "Y_shared = aesara.shared(Y)\n",
    "\n",
    "with pm.Model() as ccc_joint:\n",
    "    # ----- Priors (per series) -----\n",
    "    c     = pm.Normal(\"c\",   mu=100.0, sigma=20.0, shape=n)\n",
    "    phi   = pm.Normal(\"phi\", mu=0.0,   sigma=0.5,  shape=n)\n",
    "    omega = pm.TruncatedNormal(\"omega\", mu=100.0, sigma=20.0, lower=1e-8, shape=n)\n",
    "    alpha = pm.Beta(\"alpha\", alpha=5.0, beta=2.0, shape=n)\n",
    "    beta  = pm.Beta(\"beta\",  alpha=2.0, beta=5.0, shape=n)\n",
    "\n",
    "    # Soft stationarity: alpha + beta < 1\n",
    "    pm.Potential(\"stationarity_soft\",\n",
    "        -100.0 * at.sum(at.nnet.relu(alpha + beta - 0.999))\n",
    "    )\n",
    "\n",
    "    # ----- Constant correlation (CCC) -----\n",
    "    # Correlation-only Cholesky (unit variances): faster & stable\n",
    "    L = pm.LKJCorrCholesky(\"L\", n=n, eta=2.0)\n",
    "    R = pm.Deterministic(\"R\", L @ L.T)\n",
    "\n",
    "    # ----- AR(1)-GARCH(1,1) recursion to get mu_t and sigma_t -----\n",
    "    # Initialize sigma^2 at t=1\n",
    "    if use_seed_sigma:\n",
    "        sigma2_init = at.as_tensor_variable(sigma2_seed)            # (n,)\n",
    "    else:\n",
    "        denom = at.clip(1.0 - alpha - beta, 1e-6, 1e6)\n",
    "        sigma2_init = omega / denom                                 # (n,)\n",
    "\n",
    "    def step(y_tm1, y_tm2, sigma2_tm1, c, phi, omega, alpha, beta):\n",
    "        mu_t = c + phi * y_tm1                                      # (n,)\n",
    "        arch = (y_tm1 - c - y_tm2) ** 2                             # (n,)\n",
    "        sigma2_t = omega + alpha * arch + beta * sigma2_tm1         # (n,)\n",
    "        sigma2_t = at.clip(sigma2_t, 1e-12, 1e12)                   # guard\n",
    "        return mu_t, sigma2_t\n",
    "\n",
    "    y_tm1_seq = Y_shared[1:]     # (T-1, n) -> uses X_{t-1}\n",
    "    y_tm2_seq = Y_shared[:-1]    # (T-1, n) -> uses X_{t-2}\n",
    "\n",
    "    (mu_seq, sigma2_seq), _ = aesara.scan(\n",
    "        fn=step,\n",
    "        sequences=[y_tm1_seq, y_tm2_seq],\n",
    "        outputs_info=[None, sigma2_init],\n",
    "        non_sequences=[c, phi, omega, alpha, beta]\n",
    "    )\n",
    "    sigma_seq = at.sqrt(sigma2_seq)                                  # (T-1, n)\n",
    "\n",
    "    # Expose for inspection\n",
    "    pm.Deterministic(\"mu_seq\", mu_seq)\n",
    "    pm.Deterministic(\"sigma_seq\", sigma_seq)\n",
    "\n",
    "    # ----- Likelihood: X_t | F_{t-1} ~ N(mu_t, D_t R D_t) -----\n",
    "    # Build per-time log-likelihood with time-varying chol_t = diag(sigma_t) @ L\n",
    "    def ll_step(mu_t, sigma_t, y_t, L):\n",
    "        chol_t = at.diag(sigma_t) @ L                                # (n,n)\n",
    "        # vector MVN with time-specific covariance\n",
    "        logp_t = pm.logp(pm.MvNormal.dist(mu=mu_t, chol=chol_t), y_t)\n",
    "        return logp_t\n",
    "\n",
    "    logp_seq, _ = aesara.scan(\n",
    "        fn=ll_step,\n",
    "        sequences=[mu_seq, sigma_seq, Y_shared[1:]],\n",
    "        non_sequences=[L]\n",
    "    )\n",
    "    pm.Potential(\"likelihood\", at.sum(logp_seq))\n",
    "\n",
    "    # ----- Inference -----\n",
    "    trace = pm.sample(\n",
    "        draws=1500, tune=1500, chains=4,\n",
    "        target_accept=0.9, random_seed=123\n",
    "    )\n",
    "\n",
    "    # (Optional) posterior predictive for X given parameters would require forward simulation;\n",
    "    # PPC on this likelihood is not automatic since cov changes by t.\n",
    "\n",
    "# Quick summaries\n",
    "print(az.summary(trace, var_names=[\"c\",\"phi\",\"omega\",\"alpha\",\"beta\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6741fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae1913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7d307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCC\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def _nearest_correlation(A, tol=1e-8, max_iters=100):\n",
    "    \"\"\"\n",
    "    Higham (2002) projection to the nearest correlation matrix.\n",
    "    Ensures symmetric PD with unit diagonal.\n",
    "    \"\"\"\n",
    "    # Symmetrize\n",
    "    A = (A + A.T) / 2.0\n",
    "    n = A.shape[0]\n",
    "    X = A.copy()\n",
    "    Y = A.copy()\n",
    "    delta_S = np.zeros_like(A)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        R = X - delta_S\n",
    "        # PSD projection\n",
    "        eigvals, eigvecs = np.linalg.eigh(R)\n",
    "        eigvals_clipped = np.clip(eigvals, tol, None)\n",
    "        X = (eigvecs @ np.diag(eigvals_clipped) @ eigvecs.T)\n",
    "        X = (X + X.T) / 2.0  # re-sym\n",
    "        delta_S = X - R\n",
    "        # Reset diagonal to 1\n",
    "        np.fill_diagonal(X, 1.0)\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(X - Y, ord='fro') < 1e-10:\n",
    "            break\n",
    "        Y = X.copy()\n",
    "    # Final symmetrize\n",
    "    X = (X + X.T) / 2.0\n",
    "    np.fill_diagonal(X, 1.0)\n",
    "    return X\n",
    "\n",
    "def _pairwise_complete_corr(Z: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Correlation matrix using pairwise complete observations.\n",
    "    \"\"\"\n",
    "    # Pandas corr uses pairwise complete obs by default\n",
    "    R = Z.corr()\n",
    "    # Fill diag with 1 exactly\n",
    "    np.fill_diagonal(R.values, 1.0)\n",
    "    return R\n",
    "\n",
    "def _row_mask_complete(*dfs):\n",
    "    \"\"\"\n",
    "    Mask of rows where ALL provided dataframes have no NaNs across ALL columns.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=dfs[0].index)\n",
    "    for d in dfs:\n",
    "        mask &= d.notna().all(axis=1)\n",
    "    return mask\n",
    "\n",
    "# -----------------------------\n",
    "# CCC-GARCH core\n",
    "# -----------------------------\n",
    "\n",
    "def estimate_ccc_from_univariates(df, df_cond_mean, df_cond_vol, shrink_to_identity=0.0):\n",
    "    \"\"\"\n",
    "    Estimate CCC-GARCH correlation matrix R using standardized residuals z_{i,t}.\n",
    "    Inputs:\n",
    "        df            : DataFrame of raw series (T x N)\n",
    "        df_cond_mean  : DataFrame of conditional means X_{i,t} (T x N)\n",
    "        df_cond_vol   : DataFrame of conditional std devs sigma_{i,t} (T x N)\n",
    "        shrink_to_identity: float in [0,1]. Optional shrinkage toward I.\n",
    "                            R_hat <- (1 - s)*R_sample + s*I\n",
    "    Returns:\n",
    "        results dict with:\n",
    "            - 'R': pd.DataFrame (N x N), PD correlation matrix\n",
    "            - 'Z': standardized residuals DataFrame (T x N)\n",
    "            - 'Eps': residuals DataFrame (T x N)\n",
    "            - 'loglik': float (quasi log-likelihood, in-sample, using rows with complete data)\n",
    "            - 'H_t': dict {timestamp: (N x N) covariance matrix as pd.DataFrame}\n",
    "    \"\"\"\n",
    "    # Align and sanity-check\n",
    "    df, df_cond_mean, df_cond_vol = df.align(df_cond_mean, join='inner', axis=None)\n",
    "    df, df_cond_vol = df.align(df_cond_vol, join='inner', axis=None)\n",
    "\n",
    "    # Residuals and standardized residuals\n",
    "    Eps = df - df_cond_mean\n",
    "    Z = Eps / df_cond_vol\n",
    "\n",
    "    # Initial correlation estimate (pairwise complete)\n",
    "    R_sample = _pairwise_complete_corr(Z)\n",
    "\n",
    "    # Optional shrinkage to identity for robustness\n",
    "    if not (0.0 <= shrink_to_identity <= 1.0):\n",
    "        raise ValueError(\"shrink_to_identity must be in [0,1].\")\n",
    "    if shrink_to_identity > 0.0:\n",
    "        I = np.eye(R_sample.shape[0])\n",
    "        R_shrunk = (1.0 - shrink_to_identity) * R_sample.values + shrink_to_identity * I\n",
    "        R = pd.DataFrame(R_shrunk, index=R_sample.index, columns=R_sample.columns)\n",
    "    else:\n",
    "        R = R_sample.copy()\n",
    "\n",
    "    # Project to nearest valid correlation matrix\n",
    "    R_pd = _nearest_correlation(R.values)\n",
    "    R = pd.DataFrame(R_pd, index=R.index, columns=R.columns)\n",
    "\n",
    "    # Build per-time covariance matrices H_t = D_t R D_t\n",
    "    # Use only rows where df_cond_vol has all sigmas present (positive)\n",
    "    valid_rows = df_cond_vol.notna().all(axis=1)\n",
    "    # Guard against non-positive sigmas\n",
    "    positive_rows = (df_cond_vol > 0).all(axis=1)\n",
    "    row_mask = valid_rows & positive_rows\n",
    "\n",
    "    H_t = {}\n",
    "    cols = df.columns.tolist()\n",
    "    R_np = R.values\n",
    "    for t, row in df_cond_vol[row_mask].iterrows():\n",
    "        D = np.diag(row.values)  # diag of sigmas at time t\n",
    "        H = D @ R_np @ D\n",
    "        H_t[t] = pd.DataFrame(H, index=cols, columns=cols)\n",
    "\n",
    "    # Quasi log-likelihood using rows where we have complete data across df/Eps/sigmas\n",
    "    complete_mask = _row_mask_complete(df, df_cond_mean, df_cond_vol)\n",
    "    Z_complete = Z.loc[complete_mask]\n",
    "    Sigma_complete = df_cond_vol.loc[complete_mask]\n",
    "\n",
    "    # Log-likelihood for multivariate normal with mean = cond_mean and cov H_t\n",
    "    # l_t = -0.5 [ N log(2π) + log|H_t| + e_t' H_t^{-1} e_t ]\n",
    "    # Efficiently via Cholesky of H_t\n",
    "    const = -0.5 * Z_complete.shape[1] * np.log(2.0 * np.pi)\n",
    "    total_ll = 0.0\n",
    "    for t in Z_complete.index:\n",
    "        d = Sigma_complete.loc[t].values\n",
    "        # H_t = D R D -> log|H_t| = 2*sum(log d_i) + log|R|\n",
    "        # And e'H^{-1}e = (D^{-1} e)' R^{-1} (D^{-1} e) = z' R^{-1} z\n",
    "        # So we can reuse constant R across t\n",
    "        # Precompute Cholesky of R once\n",
    "        # (we will compute once outside the loop for efficiency)\n",
    "        pass\n",
    "    # Precompute parts for all t\n",
    "    dlogs = np.log(Sigma_complete.values).sum(axis=1)  # sum log sigmas at each t\n",
    "    # Cholesky of R\n",
    "    L = cholesky(R_np, lower=True, check_finite=True)\n",
    "    log_det_R = 2.0 * np.log(np.diag(L)).sum()\n",
    "    # Solve for z via L\n",
    "    for i, t in enumerate(Z_complete.index):\n",
    "        z = Z_complete.loc[t].values\n",
    "        # Solve L y = z => y = L^{-1} z\n",
    "        y = solve_triangular(L, z, lower=True, check_finite=False)\n",
    "        quad = y @ y  # ||y||^2\n",
    "        logdet_Ht = 2.0 * dlogs[i] + log_det_R\n",
    "        lt = const - 0.5 * (logdet_Ht + quad)\n",
    "        total_ll += lt\n",
    "\n",
    "    return {\n",
    "        \"R\": R,\n",
    "        \"Z\": Z,\n",
    "        \"Eps\": Eps,\n",
    "        \"loglik\": float(total_ll),\n",
    "        \"H_t\": H_t,\n",
    "    }\n",
    "\n",
    "def ccc_covariance_path(df_cond_vol, R):\n",
    "    \"\"\"\n",
    "    Given per-time sigmas and a fixed correlation matrix R,\n",
    "    return covariance matrices H_t = D_t R D_t for all rows with complete positive sigmas.\n",
    "    \"\"\"\n",
    "    R_np = np.asarray(R)\n",
    "    cols = df_cond_vol.columns.tolist()\n",
    "    valid_rows = df_cond_vol.notna().all(axis=1) & (df_cond_vol > 0).all(axis=1)\n",
    "    H_t = {}\n",
    "    for t, row in df_cond_vol[valid_rows].iterrows():\n",
    "        D = np.diag(row.values)\n",
    "        H = D @ R_np @ D\n",
    "        H_t[t] = pd.DataFrame(H, index=cols, columns=cols)\n",
    "    return H_t\n",
    "\n",
    "def ccc_one_step_forecast(sigma_next: pd.Series, R: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    One-step-ahead covariance forecast: H_{t+1|t} = D_{t+1} R D_{t+1}.\n",
    "    Inputs:\n",
    "        sigma_next : pd.Series indexed by series IDs with next-step sigmas (>0)\n",
    "        R          : correlation matrix (pd.DataFrame)\n",
    "    Returns:\n",
    "        H_next     : DataFrame covariance matrix\n",
    "    \"\"\"\n",
    "    sigma_next = sigma_next.dropna()\n",
    "    ids = R.index.intersection(sigma_next.index)\n",
    "    D = np.diag(sigma_next.loc[ids].values)\n",
    "    R_sub = R.loc[ids, ids].values\n",
    "    H = D @ R_sub @ D\n",
    "    return pd.DataFrame(H, index=ids, columns=ids)\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "# Assuming you already have df, df_cond_mean, df_cond_vol as described:\n",
    "# results = estimate_ccc_from_univariates(df, df_cond_mean, df_cond_vol, shrink_to_identity=0.05)\n",
    "# R_hat = results[\"R\"]          # constant correlation matrix\n",
    "# loglik = results[\"loglik\"]    # quasi log-likelihood (in-sample)\n",
    "# H_t = results[\"H_t\"]          # dict of per-date covariance matrices\n",
    "# Z = results[\"Z\"]              # standardized residuals\n",
    "# Eps = results[\"Eps\"]          # raw residuals\n",
    "#\n",
    "# # Optional: compute the entire covariance path again or forecast one-step ahead\n",
    "# H_path = ccc_covariance_path(df_cond_vol, R_hat)\n",
    "# # Suppose you’ve computed/forecasted sigma for t+1 in a Series:\n",
    "# # H_next = ccc_one_step_forecast(sigma_next_series, R_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e236d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCC\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "\n",
    "# ==========================\n",
    "# Helpers\n",
    "# ==========================\n",
    "\n",
    "def nearest_correlation(A, tol=1e-12, max_iters=100):\n",
    "    \"\"\"\n",
    "    Higham (2002) projection to the nearest correlation matrix.\n",
    "    Ensures symmetric PSD with unit diagonal. Adds tiny jitter if needed.\n",
    "    \"\"\"\n",
    "    A = (A + A.T) / 2.0\n",
    "    n = A.shape[0]\n",
    "    X = A.copy()\n",
    "    Y = A.copy()\n",
    "    delta_S = np.zeros_like(A)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        R = X - delta_S\n",
    "        # PSD projection\n",
    "        eigvals, eigvecs = np.linalg.eigh(R)\n",
    "        eigvals = np.clip(eigvals, tol, None)\n",
    "        X = (eigvecs @ np.diag(eigvals) @ eigvecs.T)\n",
    "        X = (X + X.T) / 2.0\n",
    "        delta_S = X - R\n",
    "        np.fill_diagonal(X, 1.0)\n",
    "        if np.linalg.norm(X - Y, ord='fro') < 1e-10:\n",
    "            break\n",
    "        Y = X.copy()\n",
    "    X = (X + X.T) / 2.0\n",
    "    np.fill_diagonal(X, 1.0)\n",
    "    return X\n",
    "\n",
    "def softplus(x):\n",
    "    # stable softplus\n",
    "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
    "\n",
    "def ab_from_uv(u, v):\n",
    "    \"\"\"\n",
    "    Reparameterization that enforces a>=0, b>=0, a+b<1:\n",
    "        a = exp(u) / (1 + exp(u) + exp(v))\n",
    "        b = exp(v) / (1 + exp(u) + exp(v))\n",
    "    \"\"\"\n",
    "    eu, ev = np.exp(u), np.exp(v)\n",
    "    denom = 1.0 + eu + ev\n",
    "    a = eu / denom\n",
    "    b = ev / denom\n",
    "    return a, b\n",
    "\n",
    "def loglike_dcc(z, a, b, project=True):\n",
    "    \"\"\"\n",
    "    DCC(1,1) log-likelihood contribution (multivariate part only).\n",
    "    z: (T x N) standardized residuals with no NaNs\n",
    "    Returns: total loglik, arrays of Qt (T x N x N) and Rt, and last Qt\n",
    "    \"\"\"\n",
    "    T, N = z.shape\n",
    "    # Unconditional correlation of z\n",
    "    Qbar = np.corrcoef(z.T)\n",
    "    np.fill_diagonal(Qbar, 1.0)\n",
    "\n",
    "    # Initialize Q0 as Qbar\n",
    "    Qt = np.zeros((T, N, N))\n",
    "    Rt = np.zeros_like(Qt)\n",
    "\n",
    "    Q_prev = Qbar.copy()\n",
    "\n",
    "    # Precompute to speed up\n",
    "    const = -0.5 * N * np.log(2.0 * np.pi)\n",
    "\n",
    "    total_ll = 0.0\n",
    "    for t in range(T):\n",
    "        # DCC recursion\n",
    "        zz = np.outer(z[t - 1], z[t - 1]) if t > 0 else Qbar  # use Qbar for t=0\n",
    "        Q_t = (1 - a - b) * Qbar + a * zz + b * Q_prev\n",
    "\n",
    "        # Scale to R_t\n",
    "        d = np.sqrt(np.clip(np.diag(Q_t), 1e-12, None))\n",
    "        invd = 1.0 / d\n",
    "        R_t = (invd[:, None] * Q_t) * invd[None, :]\n",
    "\n",
    "        # Numerical safety: project if needed\n",
    "        try:\n",
    "            L = cholesky(R_t, lower=True, check_finite=True)\n",
    "        except Exception:\n",
    "            if not project:\n",
    "                # add tiny jitter on the diagonal scaling to avoid failure\n",
    "                jitter = 1e-10\n",
    "                R_t = R_t + np.eye(N) * jitter\n",
    "                L = cholesky(R_t, lower=True, check_finite=False)\n",
    "            else:\n",
    "                R_t = nearest_correlation(R_t)\n",
    "                L = cholesky(R_t, lower=True, check_finite=False)\n",
    "\n",
    "        # contribution: -0.5*(log|R_t| + z_t' R_t^{-1} z_t) + const\n",
    "        log_det_Rt = 2.0 * np.log(np.diag(L)).sum()\n",
    "        y = solve_triangular(L, z[t], lower=True, check_finite=False)\n",
    "        quad = y @ y\n",
    "        lt = const - 0.5 * (log_det_Rt + quad)\n",
    "        total_ll += lt\n",
    "\n",
    "        Qt[t] = Q_t\n",
    "        Rt[t] = R_t\n",
    "        Q_prev = Q_t\n",
    "\n",
    "    return float(total_ll), Qt, Rt, Q_prev, Qbar\n",
    "\n",
    "# ==========================\n",
    "# DCC API\n",
    "# ==========================\n",
    "\n",
    "def fit_dcc_from_univariates(df, df_cond_mean, df_cond_vol,\n",
    "                             u0=np.log(0.05/0.90), v0=np.log(0.93/0.90),\n",
    "                             project=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Fit Engle's DCC(1,1) after univariate AR-GARCH:\n",
    "      z_{i,t} = (y_{i,t} - X_{i,t}) / sigma_{i,t}\n",
    "\n",
    "    Inputs:\n",
    "      df, df_cond_mean, df_cond_vol : DataFrames (aligned by date x series)\n",
    "      u0, v0 : initial values for unconstrained parameters (map -> a,b)\n",
    "      project: if True, project Rt to nearest correlation on failures\n",
    "      verbose: print optimizer status\n",
    "\n",
    "    Returns dict with:\n",
    "      'params' : {'a':..., 'b':...}\n",
    "      'loglik' : total DCC log-likelihood (multivariate part)\n",
    "      'Z'      : standardized residuals (complete-case)\n",
    "      'Qt', 'Rt': np arrays (T_eff x N x N)\n",
    "      'Qbar'   : unconditional correlation of Z\n",
    "      'H_t'    : dict mapping timestamp -> covariance matrix (DataFrame)\n",
    "      'complete_index': index used in estimation\n",
    "    \"\"\"\n",
    "    # Align and compute standardized residuals\n",
    "    df, df_cond_mean, df_cond_vol = df.align(df_cond_mean, join='inner', axis=None)\n",
    "    df, df_cond_vol = df.align(df_cond_vol, join='inner', axis=None)\n",
    "    eps = df - df_cond_mean\n",
    "    z = eps / df_cond_vol\n",
    "\n",
    "    # DCC requires complete cases across series during estimation\n",
    "    mask = z.notna().all(axis=1) & (df_cond_vol > 0).all(axis=1)\n",
    "    Z = z.loc[mask].values\n",
    "    idx = z.index[mask]\n",
    "    cols = z.columns.tolist()\n",
    "\n",
    "    if Z.shape[0] < 10:\n",
    "        raise ValueError(\"Not enough complete observations for DCC estimation.\")\n",
    "\n",
    "    # Objective = NEGATIVE log-likelihood (to minimize)\n",
    "    def obj(theta):\n",
    "        u, v = theta\n",
    "        a, b = ab_from_uv(u, v)\n",
    "        # Penalize extreme sums near 1 to help numerics\n",
    "        penalty = 1e4 * np.maximum(a + b - 0.999, 0.0)**2\n",
    "        ll, *_ = loglike_dcc(Z, a, b, project=project)\n",
    "        return -(ll) + penalty\n",
    "\n",
    "    theta0 = np.array([u0, v0])  # sensible defaults: a~0.05, b~0.93\n",
    "    res = minimize(obj, theta0, method=\"L-BFGS-B\")\n",
    "\n",
    "    if verbose:\n",
    "        print(res)\n",
    "\n",
    "    u_hat, v_hat = res.x\n",
    "    a_hat, b_hat = ab_from_uv(u_hat, v_hat)\n",
    "    ll, Qt, Rt, Q_last, Qbar = loglike_dcc(Z, a_hat, b_hat, project=project)\n",
    "\n",
    "    # Build H_t = D_t R_t D_t over the *same* complete index\n",
    "    Sigma = df_cond_vol.loc[idx].values\n",
    "    H_t = {}\n",
    "    for t, ts in enumerate(idx):\n",
    "        D = np.diag(Sigma[t])\n",
    "        H = D @ Rt[t] @ D\n",
    "        H_t[ts] = pd.DataFrame(H, index=cols, columns=cols)\n",
    "\n",
    "    return {\n",
    "        \"params\": {\"a\": float(a_hat), \"b\": float(b_hat)},\n",
    "        \"loglik\": float(ll),\n",
    "        \"Z\": pd.DataFrame(Z, index=idx, columns=cols),\n",
    "        \"Qt\": Qt,            # numpy array\n",
    "        \"Rt\": Rt,            # numpy array\n",
    "        \"Q_last\": Q_last,    # numpy array (for forecasting)\n",
    "        \"Qbar\": Qbar,        # numpy array\n",
    "        \"H_t\": H_t,          # dict of DataFrames\n",
    "        \"complete_index\": idx,\n",
    "        \"columns\": cols,\n",
    "    }\n",
    "\n",
    "def dcc_one_step_forecast(sigma_next: pd.Series,\n",
    "                          last_z: pd.Series,\n",
    "                          Q_last: np.ndarray,\n",
    "                          Qbar: np.ndarray,\n",
    "                          a: float, b: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-step-ahead forecast under DCC(1,1):\n",
    "      Q_{t+1} = (1-a-b)Qbar + a z_t z_t' + b Q_t\n",
    "      R_{t+1} = scale(Q_{t+1})\n",
    "      H_{t+1} = D_{t+1} R_{t+1} D_{t+1}\n",
    "\n",
    "    Inputs must share the same index/order of series IDs.\n",
    "    \"\"\"\n",
    "    ids = sigma_next.index.intersection(last_z.index)\n",
    "    sigma_next = sigma_next.loc[ids]\n",
    "    z_t = last_z.loc[ids].values\n",
    "    # Subset Q_last/Qbar in the same order\n",
    "    # (Assumes original order equals 'ids'. If not, reorder externally.)\n",
    "    n = len(ids)\n",
    "    Q_t = Q_last\n",
    "    if Q_t.shape[0] != n:\n",
    "        raise ValueError(\"Shape mismatch: reorder Q_last/Qbar to match series ID order.\")\n",
    "\n",
    "    Q_next = (1 - a - b) * Qbar + a * np.outer(z_t, z_t) + b * Q_t\n",
    "    d = np.sqrt(np.clip(np.diag(Q_next), 1e-12, None))\n",
    "    invd = 1.0 / d\n",
    "    R_next = (invd[:, None] * Q_next) * invd[None, :]\n",
    "    # Safety\n",
    "    try:\n",
    "        cholesky(R_next, lower=True, check_finite=True)\n",
    "    except Exception:\n",
    "        R_next = nearest_correlation(R_next)\n",
    "\n",
    "    D_next = np.diag(sigma_next.values)\n",
    "    H_next = D_next @ R_next @ D_next\n",
    "    return pd.DataFrame(H_next, index=ids, columns=ids)\n",
    "\n",
    "# ==========================\n",
    "# Example usage\n",
    "# ==========================\n",
    "# Assume you already have:\n",
    "#   df            : raw series (T x N)\n",
    "#   df_cond_mean  : conditional means X_{i,t} (T x N)\n",
    "#   df_cond_vol   : conditional std devs sigma_{i,t} (T x N)\n",
    "#\n",
    "# results = fit_dcc_from_univariates(df, df_cond_mean, df_cond_vol, verbose=True)\n",
    "# print(\"Estimated (a, b):\", results[\"params\"])\n",
    "# R_path = results[\"Rt\"]     # time-varying correlations\n",
    "# H_t    = results[\"H_t\"]    # dict of covariances by timestamp\n",
    "#\n",
    "# # Forecast example:\n",
    "# last_idx = results[\"complete_index\"][-1]\n",
    "# last_z   = results[\"Z\"].iloc[-1]                      # z_t\n",
    "# Q_last   = results[\"Q_last\"]                          # Q_t\n",
    "# Qbar     = results[\"Qbar\"]\n",
    "# a, b     = results[\"params\"][\"a\"], results[\"params\"][\"b\"]\n",
    "# # Provide sigma_{t+1} as a Series (same IDs, positive)\n",
    "# # sigma_next = pd.Series(..., index=df.columns)\n",
    "# # H_next = dcc_one_step_forecast(sigma_next, last_z, Q_last, Qbar, a, b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
